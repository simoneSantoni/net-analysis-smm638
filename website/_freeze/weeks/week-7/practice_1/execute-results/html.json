{
  "hash": "12a9ac8364c89b62e58b144c7ae889ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Community Detection in R\"\nsubtitle: \"Identifying cohesive subgroups in networks\"\nformat: \n  html:\n    code-fold: false\n    toc: true\n    toc-depth: 3\n    number-sections: true\nexecute:\n  warning: false\n  message: false\n---\n\n# Introduction\n\nCommunity detection is a fundamental technique in network analysis used to identify groups of nodes that are more densely connected to each other than to the rest of the network. In a business context, these \"communities\" often represent meaningful substructures:\n\n*   **Organizational Analytics**: Identifying informal teams or silos within a company.\n*   **Marketing**: Segmenting customers based on purchasing behavior or social influence.\n*   **Fraud Detection**: Uncovering rings of colluding actors.\n*   **Supply Chain**: Detecting tightly integrated clusters of suppliers.\n\nThis tutorial provides a comprehensive guide to detecting and analyzing these cohesive subgroups using R. We will move beyond simple execution to understand the *mechanisms* behind different algorithms, how to *compare* their results, and how to *validate* the stability of the communities found.\n\n## Learning Objectives\n\nBy the end of this tutorial, you will be able to:\n\n1.  **Implement** five distinct community detection algorithms (Louvain, Edge Betweenness, Walktrap, Spectral, and Fast Greedy).\n2.  **Evaluate** the quality of partitions using modularity and density metrics.\n3.  **Compare** results across methods to identify robust community structures.\n4.  **Visualize** communities effectively to communicate insights.\n5.  **Assess** the stability of communities to ensure findings are not artifacts of noise.\n\n## Required Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(igraph)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\n# Set seed for reproducibility\nset.seed(42)\n```\n:::\n\n\n# Loading and Exploring Network Data\n\n## Example 1: Karate Club Network\n\nWe will begin with the canonical **Zachary's Karate Club** network. This dataset documents the social interactions between 34 members of a university karate club in the 1970s.\n\n**Why this dataset?**\nIt is the \"Hello World\" of community detection because the club actually split into two factions during the study due to a conflict between the administrator (John A.) and the instructor (Mr. Hi). This provides us with a \"ground truth\" to validate our algorithms against—a luxury we rarely have in real-world business data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the karate club network\nkarate <- make_graph(\"Zachary\")\n\n# Basic network statistics\ncat(\"Network size:\", vcount(karate), \"nodes\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNetwork size: 34 nodes\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Network edges:\", ecount(karate), \"edges\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNetwork edges: 78 edges\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Network density:\", edge_density(karate), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNetwork density: 0.1390374 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Average clustering:\", transitivity(karate, type = \"average\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage clustering: 0.5879306 \n```\n\n\n:::\n:::\n\n\n## Visualizing the Original Network\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set layout for consistent visualization\nset.seed(42)\nlayout_karate <- layout_with_fr(karate)\n\n# Plot the network\nplot(karate,\n     layout = layout_karate,\n     vertex.size = 10,\n     vertex.label.cex = 0.7,\n     vertex.color = \"lightblue\",\n     vertex.frame.color = \"darkblue\",\n     edge.color = \"gray80\",\n     main = \"Zachary's Karate Club Network\")\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/viz-original-1.png){width=768}\n:::\n:::\n\n\n# Community Detection Algorithms\n\n## Method 1: Louvain Algorithm\n\nThe **Louvain algorithm** is one of the most popular methods for community detection due to its efficiency and ability to handle large networks (up to millions of nodes).\n\n### How it works\nIt is a hierarchical, greedy algorithm that optimizes **modularity** ($Q$).\n1.  **Local Moving**: Each node is moved to the community of its neighbors that yields the largest increase in modularity.\n2.  **Aggregation**: A new network is built where nodes are the communities found in step 1.\n3.  **Repeat**: Steps 1 and 2 are repeated until no further modularity increase is possible.\n\n**Pros**: Very fast; good quality partitions.\n**Cons**: Can fail to detect small communities in very large networks (resolution limit).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect communities using Louvain\nlouvain_comm <- cluster_louvain(karate)\n\n# Summary statistics\ncat(\"Number of communities:\", length(louvain_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of communities: 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Modularity:\", modularity(louvain_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModularity: 0.4197896 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Community sizes:\", sizes(louvain_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommunity sizes: 11 5 12 6 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Community membership\nprint(membership(louvain_comm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 2 2 2 1 3 3 2 1 1 1 3 3 2 1 3 1 3 1 3 4 4 4 3 4 4 3 3 4 3 3\n```\n\n\n:::\n:::\n\n\n### Visualizing Louvain Communities\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define colors for communities\ncolors <- c(\"coral\", \"lightblue\", \"lightgreen\", \"gold\", \"plum\")\n\nplot(louvain_comm, karate,\n     layout = layout_karate,\n     vertex.size = 10,\n     vertex.label.cex = 0.7,\n     edge.color = \"gray80\",\n     main = paste0(\"Louvain Algorithm\\nModularity = \", \n                   round(modularity(louvain_comm), 3)))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/viz-louvain-1.png){width=768}\n:::\n:::\n\n\n## Method 2: Edge Betweenness\n\nThe **Girvan-Newman (Edge Betweenness)** algorithm is a divisive method based on the concept of \"bridges\" in a network.\n\n### How it works\nIt relies on **edge betweenness centrality**—the number of shortest paths passing through an edge. Edges connecting different communities act as bottlenecks and have high betweenness.\n1.  Calculate betweenness for all edges.\n2.  Remove the edge with the highest betweenness.\n3.  Recalculate betweenness for remaining edges.\n4.  Repeat until no edges remain or a target number of communities is reached.\n\n**Pros**: Intuitively appealing; often very accurate for small networks.\n**Cons**: Computationally expensive ($O(N^3)$); not suitable for large graphs (>1000 nodes).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect communities using edge betweenness\neb_comm <- cluster_edge_betweenness(karate)\n\ncat(\"Number of communities:\", length(eb_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of communities: 5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Modularity:\", modularity(eb_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModularity: 0.4012985 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Community sizes:\", sizes(eb_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommunity sizes: 10 6 5 12 1 \n```\n\n\n:::\n:::\n\n\n### Visualizing Edge Betweenness Communities\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(eb_comm, karate,\n     layout = layout_karate,\n     vertex.size = 10,\n     vertex.label.cex = 0.7,\n     edge.color = \"gray80\",\n     main = paste0(\"Edge Betweenness Algorithm\\nModularity = \", \n                   round(modularity(eb_comm), 3)))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/viz-edge-betweenness-1.png){width=768}\n:::\n:::\n\n\n## Method 3: Walktrap\n\nThe **Walktrap algorithm** exploits the tendency of random walks to get \"trapped\" within densely connected parts of a graph.\n\n### How it works\nIt defines a distance between nodes based on random walks. If two nodes are in the same community, the probability of getting from one to the other in a random walk is high. The algorithm agglomeratively merges nodes/communities that minimize the squared distance between them.\n\n**Pros**: robust; works well on dense subgraphs.\n**Cons**: Slower than Louvain; complexity is approx $O(N^2 \\log N)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect communities using walktrap\nwalktrap_comm <- cluster_walktrap(karate)\n\ncat(\"Number of communities:\", length(walktrap_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of communities: 5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Modularity:\", modularity(walktrap_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModularity: 0.3532216 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Community sizes:\", sizes(walktrap_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommunity sizes: 9 7 9 4 5 \n```\n\n\n:::\n:::\n\n\n### Visualizing Walktrap Communities\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(walktrap_comm, karate,\n     layout = layout_karate,\n     vertex.size = 10,\n     vertex.label.cex = 0.7,\n     edge.color = \"gray80\",\n     main = paste0(\"Walktrap Algorithm\\nModularity = \", \n                   round(modularity(walktrap_comm), 3)))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/viz-walktrap-1.png){width=768}\n:::\n:::\n\n\n## Method 4: Leading Eigenvector (Spectral)\n\n**Spectral clustering** methods use the linear algebra properties of the graph to find partitions.\n\n### How it works\nThis specific implementation calculates the **leading eigenvector** of the modularity matrix. The signs of the elements in this eigenvector can be used to split the network into two communities. This process is repeated recursively to subdivide the network further.\n\n**Pros**: Theoretically well-grounded; effective for identifying core structures.\n**Cons**: Can be slower; sometimes struggles with very small or very disparate community sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect communities using leading eigenvector\nspectral_comm <- cluster_leading_eigen(karate)\n\ncat(\"Number of communities:\", length(spectral_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of communities: 4 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Modularity:\", modularity(spectral_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModularity: 0.3934089 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Community sizes:\", sizes(spectral_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommunity sizes: 7 12 9 6 \n```\n\n\n:::\n:::\n\n\n### Visualizing Spectral Communities\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(spectral_comm, karate,\n     layout = layout_karate,\n     vertex.size = 10,\n     vertex.label.cex = 0.7,\n     edge.color = \"gray80\",\n     main = paste0(\"Spectral Method\\nModularity = \", \n                   round(modularity(spectral_comm), 3)))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/viz-spectral-1.png){width=768}\n:::\n:::\n\n\n## Method 5: Fast Greedy\n\nThe **Fast Greedy** (Clauset-Newman-Moore) algorithm is a hierarchical agglomerative approach designed for speed.\n\n### How it works\nIt starts with every node in its own community. At each step, it merges the pair of communities that results in the largest increase in modularity. It uses sophisticated data structures to do this efficiently.\n\n**Pros**: Fast; good for large networks.\n**Cons**: Greedy nature means it can get stuck in local optima; often produces \"mega-communities\" by merging smaller ones too early.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect communities using fast greedy\nfg_comm <- cluster_fast_greedy(karate)\n\ncat(\"Number of communities:\", length(fg_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of communities: 3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Modularity:\", modularity(fg_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModularity: 0.3806706 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Community sizes:\", sizes(fg_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommunity sizes: 8 17 9 \n```\n\n\n:::\n:::\n\n\n# Comparing Methods\n\nNo single algorithm is perfect for every network. The \"best\" partition often depends on the specific characteristics of your data and your analytical goals. We will compare the methods based on:\n1.  **Modularity**: How well separated the communities are.\n2.  **Number of Communities**: Granularity of the partition.\n3.  **Agreement**: Do different methods find similar structures?\n\n## Summary Comparison Table\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create comparison data frame\ncomparison <- data.frame(\n  Method = c(\"Louvain\", \"Edge Betweenness\", \"Walktrap\", \n             \"Spectral\", \"Fast Greedy\"),\n  Communities = c(length(louvain_comm), length(eb_comm), \n                  length(walktrap_comm), length(spectral_comm), \n                  length(fg_comm)),\n  Modularity = c(modularity(louvain_comm), modularity(eb_comm),\n                 modularity(walktrap_comm), modularity(spectral_comm),\n                 modularity(fg_comm))\n)\n\n# Display table\nknitr::kable(comparison, \n             digits = 4,\n             caption = \"Comparison of Community Detection Methods\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Comparison of Community Detection Methods\n\n|Method           | Communities| Modularity|\n|:----------------|-----------:|----------:|\n|Louvain          |           4|     0.4198|\n|Edge Betweenness |           5|     0.4013|\n|Walktrap         |           5|     0.3532|\n|Spectral         |           4|     0.3934|\n|Fast Greedy      |           3|     0.3807|\n\n\n:::\n:::\n\n\n## Modularity Comparison Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(comparison, aes(x = reorder(Method, Modularity), y = Modularity)) +\n  geom_col(fill = \"steelblue\", alpha = 0.8) +\n  geom_text(aes(label = round(Modularity, 3)), \n            vjust = -0.5, size = 4) +\n  coord_flip() +\n  labs(title = \"Modularity Scores by Community Detection Method\",\n       x = \"Method\",\n       y = \"Modularity\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/modularity-plot-1.png){width=768}\n:::\n:::\n\n\n## Agreement Between Methods\n\nCalculate how similar the community assignments are across methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate normalized mutual information\ncalculate_nmi <- function(comm1, comm2) {\n  compare(comm1, comm2, method = \"nmi\")\n}\n\n# Create agreement matrix\nmethods_list <- list(\n  Louvain = louvain_comm,\n  EdgeBet = eb_comm,\n  Walktrap = walktrap_comm,\n  Spectral = spectral_comm,\n  FastGreedy = fg_comm\n)\n\n# Calculate pairwise NMI\nnmi_matrix <- matrix(0, nrow = 5, ncol = 5)\nrownames(nmi_matrix) <- colnames(nmi_matrix) <- names(methods_list)\n\nfor(i in 1:5) {\n  for(j in 1:5) {\n    nmi_matrix[i,j] <- calculate_nmi(methods_list[[i]], methods_list[[j]])\n  }\n}\n\n# Display as heatmap\nlibrary(reshape2)\nnmi_long <- melt(nmi_matrix)\n\nggplot(nmi_long, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), color = \"white\", size = 4) +\n  scale_fill_gradient2(low = \"blue\", mid = \"purple\", high = \"red\",\n                       midpoint = 0.5, limit = c(0, 1)) +\n  labs(title = \"Agreement Between Methods (NMI)\",\n       x = \"\", y = \"\", fill = \"NMI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(face = \"bold\", size = 14))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/method-agreement-1.png){width=672}\n:::\n:::\n\n\n# Assessing Community Quality\n\nModularity is a useful summary statistic, but it's abstract. To get a concrete sense of community quality, we can look at **density**.\n\n## Within vs. Between Community Density\n\nA \"good\" community should have:\n1.  High **internal density**: Nodes within the community are tightly connected.\n2.  Low **external density**: Few connections exist between different communities.\n\nWe can quantify this by calculating the ratio of internal to external density. A high ratio (>10 or >100) indicates very well-defined groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate within and between community density\nanalyze_density <- function(graph, communities) {\n  membership <- membership(communities)\n  n_comm <- max(membership)\n  \n  within_density <- numeric(n_comm)\n  \n  for(i in 1:n_comm) {\n    subgraph <- induced_subgraph(graph, which(membership == i))\n    within_density[i] <- edge_density(subgraph)\n  }\n  \n  # Calculate between community density\n  total_possible <- vcount(graph) * (vcount(graph) - 1) / 2\n  within_nodes <- sapply(1:n_comm, function(i) sum(membership == i))\n  within_possible <- sum(within_nodes * (within_nodes - 1) / 2)\n  between_possible <- total_possible - within_possible\n  \n  between_edges <- ecount(graph) - \n    sum(sapply(1:n_comm, function(i) {\n      ecount(induced_subgraph(graph, which(membership == i)))\n    }))\n  \n  between_density <- between_edges / between_possible\n  \n  list(\n    within_mean = mean(within_density),\n    within_sd = sd(within_density),\n    between = between_density,\n    ratio = mean(within_density) / between_density\n  )\n}\n\n# Analyze Louvain communities\ndensity_stats <- analyze_density(karate, louvain_comm)\n\ncat(\"Within-community density (mean):\", round(density_stats$within_mean, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWithin-community density (mean): 0.451 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Between-community density:\", round(density_stats$between, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBetween-community density: 0.051 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Ratio (within/between):\", round(density_stats$ratio, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRatio (within/between): 8.91 \n```\n\n\n:::\n:::\n\n\n## Community Size Distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data frame of community sizes for all methods\nsize_data <- data.frame(\n  Method = rep(names(methods_list), sapply(methods_list, length)),\n  Size = unlist(lapply(methods_list, sizes))\n)\n\nggplot(size_data, aes(x = Method, y = Size, fill = Method)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Community Size Distribution by Method\",\n       x = \"Method\",\n       y = \"Community Size\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(face = \"bold\", size = 14))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/size-distribution-1.png){width=768}\n:::\n:::\n\n\n# Resolution Parameter Exploration\n\nMost real-world networks have community structure at multiple scales (e.g., teams within departments within divisions).\n\nAlgorithms like Louvain allow us to tune a **resolution parameter** ($\\gamma$) to explore this hierarchy:\n*   **Lower $\\gamma$ (< 1)**: Favors larger, fewer communities (macro-structure).\n*   **Higher $\\gamma$ (> 1)**: Favors smaller, more numerous communities (micro-structure).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sweep resolution parameter\nresolutions <- seq(0.5, 2.0, by = 0.1)\nresults <- data.frame(\n  resolution = numeric(),\n  n_communities = numeric(),\n  modularity = numeric()\n)\n\nfor(res in resolutions) {\n  comm <- cluster_louvain(karate, resolution = res)\n  results <- rbind(results, data.frame(\n    resolution = res,\n    n_communities = length(comm),\n    modularity = modularity(comm)\n  ))\n}\n\n# Create plots\np1 <- ggplot(results, aes(x = resolution, y = n_communities)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(color = \"steelblue\", size = 3) +\n  labs(title = \"Resolution vs Number of Communities\",\n       x = \"Resolution Parameter\",\n       y = \"Number of Communities\") +\n  theme_minimal()\n\np2 <- ggplot(results, aes(x = resolution, y = modularity)) +\n  geom_line(color = \"coral\", size = 1.2) +\n  geom_point(color = \"coral\", size = 3) +\n  labs(title = \"Resolution vs Modularity\",\n       x = \"Resolution Parameter\",\n       y = \"Modularity\") +\n  theme_minimal()\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/resolution-sweep-1.png){width=960}\n:::\n:::\n\n\n# Example 2: Larger Network (Random Scale-Free Network)\n\nLet's apply what we've learned to a larger network. We'll generate a scale-free network using the Barabási-Albert model, which is common in many real-world networks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate a larger scale-free network\nset.seed(42)\nlarge_net <- sample_pa(n = 100, power = 1, m = 2, directed = FALSE)\n\ncat(\"Network size:\", vcount(large_net), \"nodes\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNetwork size: 100 nodes\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Network edges:\", ecount(large_net), \"edges\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNetwork edges: 197 edges\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Network density:\", round(edge_density(large_net), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNetwork density: 0.0398 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply Louvain\nlarge_comm <- cluster_louvain(large_net)\n\ncat(\"\\nCommunities detected:\", length(large_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCommunities detected: 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Modularity:\", round(modularity(large_comm), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModularity: 0.456 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Community sizes:\", sizes(large_comm), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCommunity sizes: 20 10 21 13 9 11 10 6 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize\nset.seed(42)\nplot(large_comm, large_net,\n     vertex.size = 6,\n     vertex.label = NA,\n     edge.color = \"gray90\",\n     main = paste0(\"Scale-Free Network Communities (n=100)\\n\",\n                   \"Louvain Algorithm, Modularity = \", \n                   round(modularity(large_comm), 3)))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/larger-network-1.png){width=960}\n:::\n:::\n\n\n# Hierarchical Community Structure\n\nHierarchical algorithms (like Fast Greedy, Edge Betweenness, and Walktrap) produce a **dendrogram**—a tree diagram showing how nodes are merged into communities step-by-step.\n\nThis is valuable for business analysis because it allows you to choose the \"cut\" that makes the most sense for your problem. For example, do you need 3 broad market segments or 15 niche micro-segments?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use fast greedy to get hierarchy\nfg_karate <- cluster_fast_greedy(karate)\n\n# Plot dendrogram\nplot(as.dendrogram(fg_karate),\n     main = \"Hierarchical Community Structure\\nFast Greedy Algorithm\",\n     xlab = \"Node\",\n     ylab = \"Merge Height\")\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/dendrogram-1.png){width=960}\n:::\n:::\n\n\n# Stability Analysis\n\nIn business analytics, **robustness** is critical. If removing a few random edges (simulating noise or missing data) completely changes your communities, then your findings are unstable and should not be the basis for major strategic decisions.\n\nWe can assess stability by perturbing the network and measuring the similarity (NMI) between the original and perturbed communities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to assess stability\nassess_stability <- function(graph, n_iterations = 100, \n                             removal_prop = 0.1) {\n  \n  original_comm <- cluster_louvain(graph)\n  original_membership <- membership(original_comm)\n  \n  similarities <- numeric(n_iterations)\n  \n  for(i in 1:n_iterations) {\n    # Remove random edges\n    edges_to_remove <- sample(ecount(graph), \n                               size = floor(ecount(graph) * removal_prop))\n    perturbed <- delete_edges(graph, edges_to_remove)\n    \n    # Detect communities in perturbed network\n    perturbed_comm <- cluster_louvain(perturbed)\n    \n    # Calculate similarity\n    similarities[i] <- compare(original_comm, perturbed_comm, \n                               method = \"nmi\")\n  }\n  \n  similarities\n}\n\n# Run stability analysis\nstability_scores <- assess_stability(karate, n_iterations = 50)\n\ncat(\"Mean stability (NMI):\", round(mean(stability_scores), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean stability (NMI): 0.835 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SD stability:\", round(sd(stability_scores), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSD stability: 0.097 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot\ndata.frame(NMI = stability_scores) %>%\n  ggplot(aes(x = NMI)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(stability_scores), \n             color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Community Detection Stability\",\n       subtitle = \"After removing 10% of edges (50 iterations)\",\n       x = \"Normalized Mutual Information\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = 14))\n```\n\n::: {.cell-output-display}\n![](practice_1_files/figure-html/stability-1.png){width=672}\n:::\n:::\n\n\n# Key Takeaways\n\n1.  **No \"Silver Bullet\"**: Different algorithms optimize different objective functions. Louvain is a great default for large networks, while Edge Betweenness is excellent for small, precise structural analysis.\n2.  **Modularity is a Guide, Not a Rule**: High modularity suggests good structure, but beware of the \"resolution limit\" where small, distinct communities are merged.\n3.  **Stability Matters**: Always test if your communities hold up to noise. Unstable communities are likely artifacts of the algorithm rather than real features of the data.\n4.  **Business Context is King**: A mathematically optimal partition might not be the most actionable. Visual inspection and domain knowledge are essential to validate if the communities make sense (e.g., \"Do these people actually work together?\").\n5.  **Resolution Parameters**: Algorithms like Louvain allow you to tune the \"resolution\" to find smaller or larger communities, which is useful for hierarchical analysis.\n\n# Exercises\n\n## Exercise 1: Compare Methods on Your Own Data\n\nLoad a network dataset of your choice and:\n\n1. Apply at least three different community detection methods\n2. Compare their modularity scores\n3. Visualize the results\n4. Assess agreement between methods using NMI\n\n## Exercise 2: Resolution Exploration\n\nUsing the Louvain algorithm:\n\n1. Sweep the resolution parameter from 0.3 to 2.5\n2. Create a plot showing how the number of communities changes\n3. Identify the \"elbow point\" where adding resolution stops being useful\n\n## Exercise 3: Community Characteristics\n\nFor the best community structure you find:\n\n1. Calculate within vs. between community density\n2. Identify which nodes are \"bridges\" between communities\n3. Examine node-level attributes (if available) to see if communities correspond to meaningful groups\n\n# Additional Resources\n\n- **igraph documentation**: [Community detection reference](https://igraph.org/r/doc/communities.html)\n- **Modularity paper**: Newman & Girvan (2004), *Physical Review E*\n- **Louvain algorithm**: Blondel et al. (2008), *Journal of Statistical Mechanics*\n- **Comparison of methods**: Fortunato (2010), *Physics Reports*\n\n---\n\n**Next Steps**: After mastering community detection, proceed to `practice_2.qmd` to learn about blockmodeling and role analysis.\n",
    "supporting": [
      "practice_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}