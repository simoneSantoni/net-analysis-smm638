{
  "hash": "3eb470b2429be4a979af37590f7a6d60",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Fuller's Beer Advocate Network Analysis\"\nsubtitle: \"Week 8 - Network Analytics\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    code-tools: true\nexecute:\n  freeze: auto\njupyter: python3\n---\n\n## Introduction\n\nThis notebook analyzes the BeerAdvocate forum participation data accompanying the Fuller's teaching case. We examine how beer enthusiasts allocate their attention across different beer styles, revealing community structure through network analysis.\n\n### Analysis Pipeline\n\nThe analysis proceeds through five main stages:\n\n1. **Data Loading & Network Construction**: Load user-style participation data and build a two-mode (bipartite) network\n2. **Network Projection**: Transform the bipartite network into a one-mode user similarity network\n3. **Structural Analysis**: Compute network statistics and assess small-world properties\n4. **Community Detection**: Apply modularity-based algorithms (Louvain, Label Propagation, Greedy)\n5. **Stochastic Block Modeling**: Fit a hierarchical weighted SBM using graph-tool for probabilistic community inference\n\n### Data Structure\n\nThe data represents a **two-mode (bipartite) network** where:\n\n- **Users** (1,000 nodes) participate in forum discussions\n- **Beer Styles** (20 nodes) represent categories of discussion\n- **Edges** connect users to the styles they discuss, weighted by post count\n\nWhen projected onto users, this network reveals attention-based communities—groups of enthusiasts who engage with similar styles. The projection creates an edge between two users whenever they share interest in at least one beer style, with edge weights reflecting the strength of shared attention.\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import libraries\"}\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom networkx.algorithms import bipartite\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nprint(\"NetworkX version:\", nx.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNetworkX version: 3.5\n```\n:::\n:::\n\n\n## 1. Loading the Two-Mode Network Data\n\nWe load three CSV files:\n\n- `beer_styles.csv`: Reference table of 20 beer styles\n- `ba_users.csv`: User node attributes\n- `ba_edges.csv`: User-to-style participation edges\n\n::: {#load-data .cell execution_count=2}\n``` {.python .cell-code}\n# Load beer styles reference\nstyles_df = pd.read_csv('../../../data/beerAdvocate/beer_styles.csv')\nprint(\"Beer Styles:\")\nprint(styles_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBeer Styles:\nstyle_id        style_name  community  popularity\n     S01    English Bitter          1        0.70\n     S02               ESB          1        0.60\n     S03  English Pale Ale          1        0.50\n     S04              Mild          1        0.30\n     S05      American IPA          2        0.90\n     S06        Double IPA          2        0.80\n     S07 American Pale Ale          2        0.75\n     S08       Session IPA          2        0.50\n     S09    Belgian Tripel          3        0.60\n     S10    Belgian Dubbel          3        0.55\n     S11            Saison          3        0.65\n     S12           Witbier          3        0.50\n     S13    Imperial Stout          4        0.70\n     S14            Porter          4        0.60\n     S15     Oatmeal Stout          4        0.50\n     S16     Baltic Porter          4        0.40\n     S17    German Pilsner          5        0.65\n     S18     Czech Pilsner          5        0.55\n     S19            Helles          5        0.45\n     S20       Schwarzbier          5        0.35\n```\n:::\n:::\n\n\n::: {#load-users .cell execution_count=3}\n``` {.python .cell-code}\n# Load user data\nusers_df = pd.read_csv('../../../data/beerAdvocate/ba_users.csv')\nprint(f\"\\nUsers: {len(users_df)} records\")\nprint(f\"\\nUser attributes:\")\nprint(users_df.head(10))\nprint(f\"\\nPrimary community distribution:\")\nprint(users_df['primary_community'].value_counts().sort_index())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nUsers: 1000 records\n\nUser attributes:\n  user_id  primary_community  is_bridge  n_styles  days_active  total_posts\n0   U0001                  2      False         4           91            8\n1   U0002                  1      False         6          365           92\n2   U0003                  1      False         3          103            7\n3   U0004                  2      False         4          236           17\n4   U0005                  3      False         2          252           38\n5   U0006                  2      False         2          357            5\n6   U0007                  3      False         3          450           21\n7   U0008                  1      False         3           42           90\n8   U0009                  2      False         4          318           10\n9   U0010                  5      False         2         1053           56\n\nPrimary community distribution:\nprimary_community\n1    155\n2    331\n3    220\n4    188\n5    106\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#load-edges .cell execution_count=4}\n``` {.python .cell-code}\n# Load edges (user -> style participation)\nedges_df = pd.read_csv('../../../data/beerAdvocate/ba_edges.csv')\nprint(f\"\\nEdges: {len(edges_df)} user-style connections\")\nprint(f\"\\nEdge sample:\")\nprint(edges_df.head(10))\nprint(f\"\\nPost count statistics:\")\nprint(edges_df['post_count'].describe())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEdges: 3613 user-style connections\n\nEdge sample:\n  user_id        beer_style  post_count\n0   U0001        Double IPA           3\n1   U0001           Witbier           3\n2   U0001       Session IPA           1\n3   U0001    Imperial Stout           1\n4   U0002              Mild          12\n5   U0002    English Bitter           1\n6   U0002       Session IPA          54\n7   U0002  English Pale Ale          11\n8   U0002               ESB           5\n9   U0002    German Pilsner           9\n\nPost count statistics:\ncount    3613.000000\nmean       12.459452\nstd        18.198178\nmin         1.000000\n25%         2.000000\n50%         5.000000\n75%        15.000000\nmax       181.000000\nName: post_count, dtype: float64\n```\n:::\n:::\n\n\n## 2. Creating a NetworkX Bipartite Graph\n\nWe construct the two-mode network using NetworkX, with users and beer styles as the two node sets.\n\n::: {#create-bipartite .cell execution_count=5}\n``` {.python .cell-code}\n# Create empty bipartite graph\nB = nx.Graph()\n\n# Add user nodes (bipartite=0)\nuser_ids = users_df['user_id'].tolist()\nfor _, row in users_df.iterrows():\n    B.add_node(row['user_id'],\n               bipartite=0,\n               node_type='user',\n               primary_community=row['primary_community'],\n               is_bridge=row['is_bridge'],\n               days_active=row['days_active'],\n               total_posts=row['total_posts'])\n\n# Add style nodes (bipartite=1)\nfor _, row in styles_df.iterrows():\n    B.add_node(row['style_name'],\n               bipartite=1,\n               node_type='style',\n               community=row['community'],\n               popularity=row['popularity'])\n\n# Add weighted edges\nfor _, row in edges_df.iterrows():\n    B.add_edge(row['user_id'], row['beer_style'], weight=row['post_count'])\n\n# Verify bipartite structure\nuser_nodes = {n for n, d in B.nodes(data=True) if d['bipartite'] == 0}\nstyle_nodes = {n for n, d in B.nodes(data=True) if d['bipartite'] == 1}\n\nprint(\"Bipartite Network Created\")\nprint(f\"  User nodes: {len(user_nodes)}\")\nprint(f\"  Style nodes: {len(style_nodes)}\")\nprint(f\"  Total edges: {B.number_of_edges()}\")\nprint(f\"  Is bipartite: {bipartite.is_bipartite(B)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBipartite Network Created\n  User nodes: 1000\n  Style nodes: 20\n  Total edges: 3613\n  Is bipartite: True\n```\n:::\n:::\n\n\n::: {#bipartite-stats .cell execution_count=6}\n``` {.python .cell-code}\n# Basic bipartite network statistics\nprint(\"\\nBipartite Network Statistics:\")\nprint(f\"  Network density: {nx.density(B):.4f}\")\nprint(f\"  Average degree (users): {np.mean([B.degree(n) for n in user_nodes]):.2f}\")\nprint(f\"  Average degree (styles): {np.mean([B.degree(n) for n in style_nodes]):.2f}\")\n\n# Degree distribution for styles\nstyle_degrees = [(n, B.degree(n)) for n in style_nodes]\nstyle_degrees.sort(key=lambda x: x[1], reverse=True)\nprint(\"\\nStyle engagement (degree):\")\nfor style, degree in style_degrees[:10]:\n    print(f\"  {style}: {degree} users\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBipartite Network Statistics:\n  Network density: 0.0070\n  Average degree (users): 3.61\n  Average degree (styles): 180.65\n\nStyle engagement (degree):\n  American IPA: 286 users\n  Double IPA: 280 users\n  American Pale Ale: 273 users\n  Session IPA: 233 users\n  Saison: 200 users\n  Imperial Stout: 200 users\n  ESB: 195 users\n  English Bitter: 194 users\n  Belgian Dubbel: 187 users\n  Witbier: 185 users\n```\n:::\n:::\n\n\n## 3. Projecting onto Users (One-Mode Network)\n\nWe project the bipartite network onto the user node set. In the projected network, two users are connected if they both participate in discussions about at least one common beer style. Edge weights reflect the strength of shared interests.\n\n::: {#project-users .cell execution_count=7}\n``` {.python .cell-code}\n# Project bipartite network onto users\n# Using weighted projection where weight = sum of shared style participations\nG_users = bipartite.weighted_projected_graph(B, user_nodes)\n\nprint(\"User Projection (One-Mode Network)\")\nprint(f\"  Nodes: {G_users.number_of_nodes()}\")\nprint(f\"  Edges: {G_users.number_of_edges()}\")\nprint(f\"  Density: {nx.density(G_users):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUser Projection (One-Mode Network)\n  Nodes: 1000\n  Edges: 242861\n  Density: 0.4862\n```\n:::\n:::\n\n\n::: {#projection-stats .cell execution_count=8}\n``` {.python .cell-code}\n# Analyze projected network\nprint(\"\\nProjected Network Statistics:\")\n\n# Connectivity\nif nx.is_connected(G_users):\n    print(f\"  Connected: Yes\")\n    print(f\"  Diameter: {nx.diameter(G_users)}\")\n    print(f\"  Average shortest path: {nx.average_shortest_path_length(G_users):.3f}\")\nelse:\n    largest_cc = max(nx.connected_components(G_users), key=len)\n    print(f\"  Connected: No\")\n    print(f\"  Number of components: {nx.number_connected_components(G_users)}\")\n    print(f\"  Largest component: {len(largest_cc)} nodes\")\n\n# Clustering\navg_clustering = nx.average_clustering(G_users)\nprint(f\"  Average clustering coefficient: {avg_clustering:.4f}\")\n\n# Transitivity (global clustering)\ntransitivity = nx.transitivity(G_users)\nprint(f\"  Transitivity (global clustering): {transitivity:.4f}\")\n\n# Degree statistics\ndegrees = [d for n, d in G_users.degree()]\nprint(f\"\\nDegree statistics:\")\nprint(f\"  Min: {min(degrees)}, Max: {max(degrees)}\")\nprint(f\"  Mean: {np.mean(degrees):.2f}, Std: {np.std(degrees):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nProjected Network Statistics:\n  Connected: Yes\n  Diameter: 2\n  Average shortest path: 1.514\n  Average clustering coefficient: 0.6934\n  Transitivity (global clustering): 0.6575\n\nDegree statistics:\n  Min: 160, Max: 889\n  Mean: 485.72, Std: 141.60\n```\n:::\n:::\n\n\n::: {#small-world-check .cell execution_count=9}\n``` {.python .cell-code}\n# Small-world analysis\nprint(\"\\nSmall-World Analysis:\")\nn = G_users.number_of_nodes()\nm = G_users.number_of_edges()\np = 2 * m / (n * (n - 1))  # Edge probability\n\n# Compare to random graph expectations\nexpected_clustering_random = p\nexpected_path_random = np.log(n) / np.log(n * p) if n * p > 1 else float('inf')\n\nprint(f\"  Actual clustering: {avg_clustering:.4f}\")\nprint(f\"  Expected (random): {expected_clustering_random:.4f}\")\nprint(f\"  Clustering ratio: {avg_clustering / expected_clustering_random:.2f}x\")\n\nif nx.is_connected(G_users):\n    actual_path = nx.average_shortest_path_length(G_users)\n    print(f\"\\n  Actual avg path length: {actual_path:.3f}\")\n    print(f\"  Expected (random): {expected_path_random:.3f}\")\n\n    # Small-world coefficient (sigma)\n    sigma = (avg_clustering / expected_clustering_random) / (actual_path / expected_path_random)\n    print(f\"\\n  Small-world coefficient (σ): {sigma:.2f}\")\n    print(f\"  (σ > 1 indicates small-world structure)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSmall-World Analysis:\n  Actual clustering: 0.6934\n  Expected (random): 0.4862\n  Clustering ratio: 1.43x\n\n  Actual avg path length: 1.514\n  Expected (random): 1.117\n\n  Small-world coefficient (σ): 1.05\n  (σ > 1 indicates small-world structure)\n```\n:::\n:::\n\n\n## 4. Network Visualization\n\nWe visualize the projected user network, coloring nodes by their primary community affiliation.\n\n::: {#viz-prep .cell execution_count=10}\n``` {.python .cell-code}\n# Prepare node colors based on primary community\ncommunity_colors = {\n    1: '#E41A1C',  # Red - Traditional British\n    2: '#377EB8',  # Blue - American Craft\n    3: '#4DAF4A',  # Green - Belgian\n    4: '#984EA3',  # Purple - Dark/Roasted\n    5: '#FF7F00',  # Orange - Lager\n}\n\nnode_colors = []\nfor node in G_users.nodes():\n    comm = users_df[users_df['user_id'] == node]['primary_community'].values[0]\n    node_colors.append(community_colors[comm])\n\n# Identify bridge users for highlighting\nbridge_users = set(users_df[users_df['is_bridge'] == True]['user_id'])\nnode_sizes = [50 if n in bridge_users else 20 for n in G_users.nodes()]\n```\n:::\n\n\n::: {#cell-viz-spring .cell execution_count=11}\n``` {.python .cell-code}\n# Spring layout visualization (sample for readability)\n# For large networks, we sample nodes\nsample_size = min(300, G_users.number_of_nodes())\nsample_nodes = list(G_users.nodes())[:sample_size]\nG_sample = G_users.subgraph(sample_nodes)\n\n# Get colors for sample\nsample_colors = [node_colors[list(G_users.nodes()).index(n)] for n in G_sample.nodes()]\nsample_sizes = [node_sizes[list(G_users.nodes()).index(n)] for n in G_sample.nodes()]\n\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Compute layout\npos = nx.spring_layout(G_sample, k=0.3, iterations=50, seed=42)\n\n# Draw edges with low alpha\nnx.draw_networkx_edges(G_sample, pos, alpha=0.05, edge_color='gray', ax=ax)\n\n# Draw nodes\nnx.draw_networkx_nodes(G_sample, pos, node_color=sample_colors,\n                       node_size=sample_sizes, alpha=0.7, ax=ax)\n\n# Legend\nlegend_patches = [\n    mpatches.Patch(color=community_colors[1], label='Traditional British'),\n    mpatches.Patch(color=community_colors[2], label='American Craft'),\n    mpatches.Patch(color=community_colors[3], label='Belgian'),\n    mpatches.Patch(color=community_colors[4], label='Dark/Roasted'),\n    mpatches.Patch(color=community_colors[5], label='Lager'),\n]\nax.legend(handles=legend_patches, loc='upper left', fontsize=10)\n\nax.set_title(f'Beer Enthusiast Network (sample of {sample_size} users)', fontsize=14)\nax.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![User network with spring layout (colored by primary community)](network_analysis_files/figure-html/viz-spring-output-1.png){#viz-spring width=1142 height=949}\n:::\n:::\n\n\n::: {#cell-viz-degree-dist .cell execution_count=12}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Degree histogram\ndegrees = [d for n, d in G_users.degree()]\naxes[0].hist(degrees, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0].set_xlabel('Degree', fontsize=12)\naxes[0].set_ylabel('Frequency', fontsize=12)\naxes[0].set_title('Degree Distribution', fontsize=14)\naxes[0].axvline(np.mean(degrees), color='red', linestyle='--', label=f'Mean: {np.mean(degrees):.0f}')\naxes[0].legend()\n\n# Log-log degree distribution\ndegree_counts = Counter(degrees)\nx = list(degree_counts.keys())\ny = list(degree_counts.values())\naxes[1].scatter(x, y, alpha=0.6, color='steelblue')\naxes[1].set_xscale('log')\naxes[1].set_yscale('log')\naxes[1].set_xlabel('Degree (log)', fontsize=12)\naxes[1].set_ylabel('Frequency (log)', fontsize=12)\naxes[1].set_title('Degree Distribution (Log-Log)', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Degree distribution of the projected user network](network_analysis_files/figure-html/viz-degree-dist-output-1.png){#viz-degree-dist width=1335 height=468}\n:::\n:::\n\n\n## 5. Community Detection Algorithms\n\nWe apply several community detection algorithms to the projected network and compare their results.\n\n### 5.1 Louvain Algorithm\n\n::: {#louvain .cell execution_count=13}\n``` {.python .cell-code}\n# Louvain community detection\nlouvain_communities = nx.community.louvain_communities(G_users, seed=42)\n\nprint(\"Louvain Community Detection\")\nprint(f\"  Number of communities: {len(louvain_communities)}\")\nprint(f\"\\n  Community sizes:\")\nfor i, comm in enumerate(sorted(louvain_communities, key=len, reverse=True)):\n    print(f\"    Community {i+1}: {len(comm)} users\")\n\n# Calculate modularity\nlouvain_modularity = nx.community.modularity(G_users, louvain_communities)\nprint(f\"\\n  Modularity: {louvain_modularity:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLouvain Community Detection\n  Number of communities: 5\n\n  Community sizes:\n    Community 1: 306 users\n    Community 2: 243 users\n    Community 3: 200 users\n    Community 4: 184 users\n    Community 5: 67 users\n\n  Modularity: 0.2742\n```\n:::\n:::\n\n\n::: {#louvain-analysis .cell execution_count=14}\n``` {.python .cell-code}\n# Analyze community composition\nprint(\"\\nLouvain Community Composition (by original community):\")\nfor i, comm in enumerate(sorted(louvain_communities, key=len, reverse=True)[:5]):\n    comm_users = users_df[users_df['user_id'].isin(comm)]\n    primary_dist = comm_users['primary_community'].value_counts()\n    dominant = primary_dist.idxmax()\n    dominant_pct = primary_dist.max() / len(comm) * 100\n\n    print(f\"\\n  Detected Community {i+1} ({len(comm)} users):\")\n    print(f\"    Dominant original community: {dominant} ({dominant_pct:.1f}%)\")\n    print(f\"    Distribution: {dict(primary_dist.sort_index())}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLouvain Community Composition (by original community):\n\n  Detected Community 1 (306 users):\n    Dominant original community: 2 (88.9%)\n    Distribution: {1: np.int64(3), 2: np.int64(272), 3: np.int64(6), 4: np.int64(15), 5: np.int64(10)}\n\n  Detected Community 2 (243 users):\n    Dominant original community: 3 (81.9%)\n    Distribution: {1: np.int64(7), 2: np.int64(16), 3: np.int64(199), 4: np.int64(8), 5: np.int64(13)}\n\n  Detected Community 3 (200 users):\n    Dominant original community: 1 (70.0%)\n    Distribution: {1: np.int64(140), 2: np.int64(25), 3: np.int64(7), 4: np.int64(7), 5: np.int64(21)}\n\n  Detected Community 4 (184 users):\n    Dominant original community: 4 (84.8%)\n    Distribution: {1: np.int64(5), 2: np.int64(16), 3: np.int64(6), 4: np.int64(156), 5: np.int64(1)}\n\n  Detected Community 5 (67 users):\n    Dominant original community: 5 (91.0%)\n    Distribution: {2: np.int64(2), 3: np.int64(2), 4: np.int64(2), 5: np.int64(61)}\n```\n:::\n:::\n\n\n### 5.2 Label Propagation\n\n::: {#label-prop .cell execution_count=15}\n``` {.python .cell-code}\n# Label propagation\nlabel_prop_communities = list(nx.community.label_propagation_communities(G_users))\n\nprint(\"Label Propagation Community Detection\")\nprint(f\"  Number of communities: {len(label_prop_communities)}\")\nprint(f\"\\n  Community sizes (top 10):\")\nfor i, comm in enumerate(sorted(label_prop_communities, key=len, reverse=True)[:10]):\n    print(f\"    Community {i+1}: {len(comm)} users\")\n\n# Calculate modularity\nlabel_prop_modularity = nx.community.modularity(G_users, label_prop_communities)\nprint(f\"\\n  Modularity: {label_prop_modularity:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLabel Propagation Community Detection\n  Number of communities: 1\n\n  Community sizes (top 10):\n    Community 1: 1000 users\n\n  Modularity: 0.0000\n```\n:::\n:::\n\n\n### 5.3 Greedy Modularity Optimization\n\n::: {#greedy-modularity .cell execution_count=16}\n``` {.python .cell-code}\n# Greedy modularity optimization\ngreedy_communities = list(nx.community.greedy_modularity_communities(G_users))\n\nprint(\"Greedy Modularity Optimization\")\nprint(f\"  Number of communities: {len(greedy_communities)}\")\nprint(f\"\\n  Community sizes:\")\nfor i, comm in enumerate(sorted(greedy_communities, key=len, reverse=True)):\n    print(f\"    Community {i+1}: {len(comm)} users\")\n\n# Calculate modularity\ngreedy_modularity = nx.community.modularity(G_users, greedy_communities)\nprint(f\"\\n  Modularity: {greedy_modularity:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGreedy Modularity Optimization\n  Number of communities: 4\n\n  Community sizes:\n    Community 1: 351 users\n    Community 2: 312 users\n    Community 3: 275 users\n    Community 4: 62 users\n\n  Modularity: 0.1387\n```\n:::\n:::\n\n\n### 5.4 Algorithm Comparison\n\n::: {#cell-compare-algorithms .cell execution_count=17}\n``` {.python .cell-code}\n# Summary comparison\nresults = {\n    'Algorithm': ['Louvain', 'Label Propagation', 'Greedy Modularity'],\n    'Communities': [len(louvain_communities), len(label_prop_communities), len(greedy_communities)],\n    'Modularity': [louvain_modularity, label_prop_modularity, greedy_modularity],\n    'Largest Community': [\n        max(len(c) for c in louvain_communities),\n        max(len(c) for c in label_prop_communities),\n        max(len(c) for c in greedy_communities)\n    ]\n}\ncomparison_df = pd.DataFrame(results)\nprint(\"\\nAlgorithm Comparison:\")\nprint(comparison_df.to_string(index=False))\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Modularity comparison\ncolors = ['#2ecc71', '#3498db', '#e74c3c']\naxes[0].bar(results['Algorithm'], results['Modularity'], color=colors, edgecolor='black')\naxes[0].set_ylabel('Modularity', fontsize=12)\naxes[0].set_title('Modularity by Algorithm', fontsize=14)\naxes[0].set_ylim(0, max(results['Modularity']) * 1.2)\nfor i, v in enumerate(results['Modularity']):\n    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=11)\n\n# Community count comparison\naxes[1].bar(results['Algorithm'], results['Communities'], color=colors, edgecolor='black')\naxes[1].set_ylabel('Number of Communities', fontsize=12)\naxes[1].set_title('Communities Detected', fontsize=14)\nfor i, v in enumerate(results['Communities']):\n    axes[1].text(i, v + 0.5, str(v), ha='center', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAlgorithm Comparison:\n        Algorithm  Communities  Modularity  Largest Community\n          Louvain            5    0.274228                306\nLabel Propagation            1    0.000000               1000\nGreedy Modularity            4    0.138717                351\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Comparison of community detection algorithms](network_analysis_files/figure-html/compare-algorithms-output-2.png){#compare-algorithms width=1143 height=471}\n:::\n:::\n\n\n## 6. Results Summary\n\n::: {#summary .cell execution_count=18}\n``` {.python .cell-code}\nprint(\"=\" * 70)\nprint(\"NETWORK ANALYSIS SUMMARY\")\nprint(\"=\" * 70)\n\nprint(\"\\n1. BIPARTITE NETWORK\")\nprint(f\"   - Users: {len(user_nodes)}\")\nprint(f\"   - Beer Styles: {len(style_nodes)}\")\nprint(f\"   - Edges: {B.number_of_edges()}\")\n\nprint(\"\\n2. PROJECTED USER NETWORK\")\nprint(f\"   - Nodes: {G_users.number_of_nodes()}\")\nprint(f\"   - Edges: {G_users.number_of_edges()}\")\nprint(f\"   - Density: {nx.density(G_users):.4f}\")\nprint(f\"   - Avg Clustering: {avg_clustering:.4f}\")\nif nx.is_connected(G_users):\n    print(f\"   - Avg Path Length: {nx.average_shortest_path_length(G_users):.3f}\")\n    print(f\"   - Diameter: {nx.diameter(G_users)}\")\n\nprint(\"\\n3. SMALL-WORLD PROPERTIES\")\nprint(f\"   - Clustering ratio (vs random): {avg_clustering / expected_clustering_random:.2f}x\")\nprint(f\"   - Network exhibits small-world structure: {'Yes' if avg_clustering / expected_clustering_random > 1 else 'No'}\")\n\nprint(\"\\n4. COMMUNITY DETECTION\")\nprint(f\"   - Best algorithm (by modularity): {comparison_df.loc[comparison_df['Modularity'].idxmax(), 'Algorithm']}\")\nprint(f\"   - Best modularity score: {comparison_df['Modularity'].max():.4f}\")\nprint(f\"   - Consensus community count: ~{int(np.median(results['Communities']))}\")\n\nprint(\"\\n5. KEY INSIGHTS\")\nprint(\"   - The network shows clear community structure aligned with beer style families\")\nprint(\"   - High clustering indicates users form cohesive interest groups\")\nprint(\"   - Short path lengths suggest bridging connections across communities\")\nprint(\"   - Community detection recovers the underlying style-based segmentation\")\nprint(\"=\" * 70)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nNETWORK ANALYSIS SUMMARY\n======================================================================\n\n1. BIPARTITE NETWORK\n   - Users: 1000\n   - Beer Styles: 20\n   - Edges: 3613\n\n2. PROJECTED USER NETWORK\n   - Nodes: 1000\n   - Edges: 242861\n   - Density: 0.4862\n   - Avg Clustering: 0.6934\n   - Avg Path Length: 1.514\n   - Diameter: 2\n\n3. SMALL-WORLD PROPERTIES\n   - Clustering ratio (vs random): 1.43x\n   - Network exhibits small-world structure: Yes\n\n4. COMMUNITY DETECTION\n   - Best algorithm (by modularity): Louvain\n   - Best modularity score: 0.2742\n   - Consensus community count: ~4\n\n5. KEY INSIGHTS\n   - The network shows clear community structure aligned with beer style families\n   - High clustering indicates users form cohesive interest groups\n   - Short path lengths suggest bridging connections across communities\n   - Community detection recovers the underlying style-based segmentation\n======================================================================\n```\n:::\n:::\n\n\n## 7. Writing the Projected Network to File\n\nWe save the projected user network in multiple formats for further analysis.\n\n::: {#write-network .cell execution_count=19}\n``` {.python .cell-code}\n# Create node attributes dataframe for the projected network\nnode_attrs = []\nfor node in G_users.nodes():\n    user_data = users_df[users_df['user_id'] == node].iloc[0]\n\n    # Find Louvain community assignment\n    louvain_comm = None\n    for i, comm in enumerate(louvain_communities):\n        if node in comm:\n            louvain_comm = i\n            break\n\n    node_attrs.append({\n        'id': node,\n        'primary_community': user_data['primary_community'],\n        'is_bridge': user_data['is_bridge'],\n        'days_active': user_data['days_active'],\n        'total_posts': user_data['total_posts'],\n        'degree': G_users.degree(node),\n        'louvain_community': louvain_comm\n    })\n\nnodes_export_df = pd.DataFrame(node_attrs)\nnodes_export_df.to_csv('../../../data/beerAdvocate/projected_nodes.csv', index=False)\nprint(\"Saved: projected_nodes.csv\")\nprint(nodes_export_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSaved: projected_nodes.csv\n      id  primary_community  is_bridge  days_active  total_posts  degree  \\\n0  U0363                  2      False         2077           28     624   \n1  U0113                  2       True          101            5     385   \n2  U0184                  2      False          154           10     369   \n3  U0590                  2       True          349            7     341   \n4  U0269                  2      False           44            8     615   \n\n   louvain_community  \n0                  0  \n1                  2  \n2                  4  \n3                  2  \n4                  0  \n```\n:::\n:::\n\n\n::: {#write-edges .cell execution_count=20}\n``` {.python .cell-code}\n# Export edges with weights\nedges_export = []\nfor u, v, d in G_users.edges(data=True):\n    edges_export.append({\n        'source': u,\n        'target': v,\n        'weight': d.get('weight', 1)\n    })\n\nedges_export_df = pd.DataFrame(edges_export)\nedges_export_df.to_csv('../../../data/beerAdvocate/projected_edges.csv', index=False)\nprint(f\"\\nSaved: projected_edges.csv ({len(edges_export_df)} edges)\")\nprint(edges_export_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSaved: projected_edges.csv (242861 edges)\n  source target  weight\n0  U0363  U0269       4\n1  U0363  U0206       4\n2  U0363  U0335       3\n3  U0363  U0195       1\n4  U0363  U0151       3\n```\n:::\n:::\n\n\n::: {#write-graphml .cell execution_count=21}\n``` {.python .cell-code}\n# Save as GraphML for compatibility with other tools\nnx.write_graphml(G_users, '../../../data/beerAdvocate/user_network.graphml')\nprint(\"\\nSaved: user_network.graphml\")\n\n# Also save edge list format\nnx.write_edgelist(G_users, '../../../data/beerAdvocate/user_network.edgelist', data=['weight'])\nprint(\"Saved: user_network.edgelist\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSaved: user_network.graphml\nSaved: user_network.edgelist\n```\n:::\n:::\n\n\n## 8. Reading the Network Back\n\nWe verify the saved files by reading them back and reconstructing the network.\n\n::: {#read-network .cell execution_count=22}\n``` {.python .cell-code}\n# Read from CSV files\nnodes_read = pd.read_csv('../../../data/beerAdvocate/projected_nodes.csv')\nedges_read = pd.read_csv('../../../data/beerAdvocate/projected_edges.csv')\n\nprint(\"Reading network from CSV files...\")\nprint(f\"  Nodes: {len(nodes_read)}\")\nprint(f\"  Edges: {len(edges_read)}\")\n\n# Reconstruct NetworkX graph\nG_reconstructed = nx.Graph()\n\n# Add nodes with attributes\nfor _, row in nodes_read.iterrows():\n    G_reconstructed.add_node(row['id'],\n                             primary_community=row['primary_community'],\n                             is_bridge=row['is_bridge'],\n                             degree=row['degree'],\n                             louvain_community=row['louvain_community'])\n\n# Add edges with weights\nfor _, row in edges_read.iterrows():\n    G_reconstructed.add_edge(row['source'], row['target'], weight=row['weight'])\n\nprint(f\"\\nReconstructed graph:\")\nprint(f\"  Nodes: {G_reconstructed.number_of_nodes()}\")\nprint(f\"  Edges: {G_reconstructed.number_of_edges()}\")\nprint(f\"  Matches original: {G_reconstructed.number_of_nodes() == G_users.number_of_nodes() and G_reconstructed.number_of_edges() == G_users.number_of_edges()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading network from CSV files...\n  Nodes: 1000\n  Edges: 242861\n\nReconstructed graph:\n  Nodes: 1000\n  Edges: 242861\n  Matches original: True\n```\n:::\n:::\n\n\n::: {#read-graphml .cell execution_count=23}\n``` {.python .cell-code}\n# Read from GraphML\nG_from_graphml = nx.read_graphml('../../../data/beerAdvocate/user_network.graphml')\nprint(f\"\\nRead from GraphML:\")\nprint(f\"  Nodes: {G_from_graphml.number_of_nodes()}\")\nprint(f\"  Edges: {G_from_graphml.number_of_edges()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRead from GraphML:\n  Nodes: 1000\n  Edges: 242861\n```\n:::\n:::\n\n\n## 9. Graph-Tool Network Instantiation\n\nWe now convert the network to graph-tool format for advanced analysis. Graph-tool is a highly efficient Python library for network analysis that provides sophisticated algorithms including stochastic block models.\n\n::: {#graphtool-setup .cell execution_count=24}\n``` {.python .cell-code}\n# Import graph-tool\ntry:\n    import graph_tool.all as gt\n    GRAPH_TOOL_AVAILABLE = True\n    print(f\"graph-tool version: {gt.__version__}\")\nexcept ImportError:\n    GRAPH_TOOL_AVAILABLE = False\n    print(\"graph-tool is not installed.\")\n    print(\"To install: conda install -c conda-forge graph-tool\")\n    print(\"Or see: https://graph-tool.skewed.de/\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngraph-tool version: 2.98 (commit 9c32966d, )\n```\n:::\n:::\n\n\n::: {#graphtool-convert .cell execution_count=25}\n``` {.python .cell-code}\nif GRAPH_TOOL_AVAILABLE:\n    # Create graph-tool graph\n    g = gt.Graph(directed=False)\n\n    # Create vertex property map for node IDs\n    v_id = g.new_vertex_property(\"string\")\n    v_primary_comm = g.new_vertex_property(\"int\")\n    v_is_bridge = g.new_vertex_property(\"bool\")\n    v_louvain = g.new_vertex_property(\"int\")\n\n    # Create edge property map for weights\n    e_weight = g.new_edge_property(\"double\")\n\n    # Map from node ID to vertex\n    node_to_vertex = {}\n\n    # Add vertices\n    for _, row in nodes_read.iterrows():\n        v = g.add_vertex()\n        node_to_vertex[row['id']] = v\n        v_id[v] = row['id']\n        v_primary_comm[v] = int(row['primary_community'])\n        v_is_bridge[v] = bool(row['is_bridge'])\n        v_louvain[v] = int(row['louvain_community'])\n\n    # Add edges\n    for _, row in edges_read.iterrows():\n        e = g.add_edge(node_to_vertex[row['source']],\n                       node_to_vertex[row['target']])\n        e_weight[e] = row['weight']\n\n    # Set as internal properties\n    g.vertex_properties[\"id\"] = v_id\n    g.vertex_properties[\"primary_community\"] = v_primary_comm\n    g.vertex_properties[\"is_bridge\"] = v_is_bridge\n    g.vertex_properties[\"louvain_community\"] = v_louvain\n    g.edge_properties[\"weight\"] = e_weight\n\n    print(\"Graph-tool network created:\")\n    print(f\"  Vertices: {g.num_vertices()}\")\n    print(f\"  Edges: {g.num_edges()}\")\nelse:\n    print(\"Skipping graph-tool conversion (not installed)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGraph-tool network created:\n  Vertices: 1000\n  Edges: 242861\n```\n:::\n:::\n\n\n## 10. Stochastic Weighted Block Model\n\nThe Stochastic Block Model (SBM) is a generative model for networks that assumes nodes belong to groups (blocks), and the probability of an edge between two nodes depends only on their group memberships.\n\nGraph-tool provides a **hierarchical nested SBM** implementation that:\n\n- Discovers multi-level community structure automatically\n- Handles edge weights natively\n- Uses Bayesian inference to avoid overfitting\n- Provides statistically principled model selection\n\nThe weighted SBM approach follows [Peixoto (2018)](https://doi.org/10.1103/PhysRevE.97.012306), which extends the nonparametric SBM framework to incorporate edge weights through microcanonical distributions (here, `real-exponential` for continuous positive weights).\n\n::: {.callout-note collapse=\"true\"}\n## Reference\n\nPeixoto, T. P. (2018). Nonparametric weighted stochastic block models. *Physical Review E*, 97(1), 012306. [doi:10.1103/PhysRevE.97.012306](https://doi.org/10.1103/PhysRevE.97.012306) | [arXiv:1708.01432](https://arxiv.org/abs/1708.01432)\n:::\n\n::: {#sbm-fit .cell execution_count=26}\n``` {.python .cell-code}\nif GRAPH_TOOL_AVAILABLE:\n    print(\"Fitting Hierarchical Nested Stochastic Block Model...\")\n    print(\"(This may take a few minutes for large networks)\\n\")\n\n    # Fit weighted hierarchical nested SBM using minimize_nested_blockmodel_dl\n    # This finds optimal hierarchical partition by minimizing description length\n    # The nested model automatically discovers multiple levels of community structure\n    nested_state = gt.minimize_nested_blockmodel_dl(\n        g,\n        state_args=dict(recs=[e_weight], rec_types=[\"real-exponential\"])\n    )\n\n    print(f\"Initial description length: {nested_state.entropy():.2f}\")\n\n    # Improve solution with merge-split MCMC sweeps\n    print(\"\\nRefining solution with merge-split MCMC...\")\n    for i in range(100):\n        ret = nested_state.multiflip_mcmc_sweep(niter=10, beta=np.inf)\n\n    print(f\"Refined description length: {nested_state.entropy():.2f}\")\n\n    # Get the hierarchy of block states\n    levels = nested_state.get_levels()\n    print(f\"\\nHierarchical SBM Results:\")\n    print(f\"  Number of hierarchy levels: {len(levels)}\")\n\n    print(f\"\\n  Hierarchy structure:\")\n    for i, level_state in enumerate(levels):\n        level_blocks = level_state.get_blocks()\n        level_block_counts = Counter(level_blocks[v] for v in level_state.g.vertices())\n        n_level_blocks = len(level_block_counts)\n        print(f\"    Level {i}: {level_state.g.num_vertices()} nodes → {n_level_blocks} blocks\")\n\n    # Get bottom-level (finest) block assignments for nodes\n    bottom_state = levels[0]\n    blocks = bottom_state.get_blocks()\n\n    # Count blocks at bottom level\n    block_counts = Counter(blocks[v] for v in g.vertices())\n    n_blocks = len(block_counts)\n\n    print(f\"\\n  Bottom-level block sizes:\")\n    for block_id, count in sorted(block_counts.items(), key=lambda x: -x[1])[:10]:\n        print(f\"    Block {block_id}: {count} nodes\")\n    if n_blocks > 10:\n        print(f\"    ... and {n_blocks - 10} more blocks\")\nelse:\n    print(\"Skipping SBM (graph-tool not installed)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting Hierarchical Nested Stochastic Block Model...\n(This may take a few minutes for large networks)\n\nInitial description length: 656073.00\n\nRefining solution with merge-split MCMC...\nRefined description length: 653166.22\n\nHierarchical SBM Results:\n  Number of hierarchy levels: 11\n\n  Hierarchy structure:\n    Level 0: 1000 nodes → 45 blocks\n    Level 1: 1000 nodes → 25 blocks\n    Level 2: 49 nodes → 11 blocks\n    Level 3: 13 nodes → 5 blocks\n    Level 4: 7 nodes → 1 blocks\n    Level 5: 6 nodes → 1 blocks\n    Level 6: 1 nodes → 1 blocks\n    Level 7: 1 nodes → 1 blocks\n    Level 8: 1 nodes → 1 blocks\n    Level 9: 1 nodes → 1 blocks\n    Level 10: 1 nodes → 1 blocks\n\n  Bottom-level block sizes:\n    Block 512: 61 nodes\n    Block 483: 52 nodes\n    Block 896: 47 nodes\n    Block 921: 46 nodes\n    Block 554: 39 nodes\n    Block 160: 37 nodes\n    Block 886: 37 nodes\n    Block 38: 31 nodes\n    Block 205: 28 nodes\n    Block 371: 27 nodes\n    ... and 35 more blocks\n```\n:::\n:::\n\n\n::: {#sbm-analysis .cell execution_count=27}\n``` {.python .cell-code}\nif GRAPH_TOOL_AVAILABLE:\n    # Analyze correspondence between SBM blocks and original communities\n    print(\"\\nSBM Block Composition (by original community):\")\n\n    # Create mapping\n    sbm_assignments = {}\n    for v in g.vertices():\n        node_id = v_id[v]\n        block = blocks[v]\n        orig_comm = v_primary_comm[v]\n        if block not in sbm_assignments:\n            sbm_assignments[block] = []\n        sbm_assignments[block].append(orig_comm)\n\n    # Print composition\n    for block_id in sorted(sbm_assignments.keys()):\n        members = sbm_assignments[block_id]\n        comm_dist = Counter(members)\n        dominant = comm_dist.most_common(1)[0]\n        print(f\"\\n  Block {block_id} ({len(members)} nodes):\")\n        print(f\"    Dominant community: {dominant[0]} ({100*dominant[1]/len(members):.1f}%)\")\n        print(f\"    Distribution: {dict(sorted(comm_dist.items()))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSBM Block Composition (by original community):\n\n  Block 9 (26 nodes):\n    Dominant community: 2 (53.8%)\n    Distribution: {2: 14, 3: 3, 4: 8, 5: 1}\n\n  Block 11 (12 nodes):\n    Dominant community: 5 (33.3%)\n    Distribution: {1: 1, 3: 3, 4: 4, 5: 4}\n\n  Block 22 (12 nodes):\n    Dominant community: 5 (50.0%)\n    Distribution: {2: 5, 3: 1, 5: 6}\n\n  Block 38 (31 nodes):\n    Dominant community: 4 (74.2%)\n    Distribution: {2: 4, 3: 4, 4: 23}\n\n  Block 83 (22 nodes):\n    Dominant community: 2 (77.3%)\n    Distribution: {2: 17, 3: 5}\n\n  Block 122 (18 nodes):\n    Dominant community: 1 (94.4%)\n    Distribution: {1: 17, 5: 1}\n\n  Block 149 (16 nodes):\n    Dominant community: 2 (37.5%)\n    Distribution: {1: 2, 2: 6, 3: 3, 4: 5}\n\n  Block 160 (37 nodes):\n    Dominant community: 2 (48.6%)\n    Distribution: {1: 7, 2: 18, 3: 2, 4: 10}\n\n  Block 188 (16 nodes):\n    Dominant community: 5 (43.8%)\n    Distribution: {1: 3, 3: 6, 5: 7}\n\n  Block 205 (28 nodes):\n    Dominant community: 5 (100.0%)\n    Distribution: {5: 28}\n\n  Block 242 (19 nodes):\n    Dominant community: 3 (78.9%)\n    Distribution: {1: 1, 2: 3, 3: 15}\n\n  Block 255 (26 nodes):\n    Dominant community: 3 (92.3%)\n    Distribution: {2: 2, 3: 24}\n\n  Block 267 (16 nodes):\n    Dominant community: 2 (43.8%)\n    Distribution: {1: 5, 2: 7, 5: 4}\n\n  Block 290 (14 nodes):\n    Dominant community: 2 (50.0%)\n    Distribution: {2: 7, 4: 6, 5: 1}\n\n  Block 324 (16 nodes):\n    Dominant community: 4 (87.5%)\n    Distribution: {3: 1, 4: 14, 5: 1}\n\n  Block 362 (12 nodes):\n    Dominant community: 2 (91.7%)\n    Distribution: {2: 11, 3: 1}\n\n  Block 371 (27 nodes):\n    Dominant community: 2 (55.6%)\n    Distribution: {1: 8, 2: 15, 3: 1, 4: 1, 5: 2}\n\n  Block 438 (16 nodes):\n    Dominant community: 3 (93.8%)\n    Distribution: {2: 1, 3: 15}\n\n  Block 447 (15 nodes):\n    Dominant community: 3 (93.3%)\n    Distribution: {3: 14, 5: 1}\n\n  Block 474 (21 nodes):\n    Dominant community: 2 (95.2%)\n    Distribution: {2: 20, 5: 1}\n\n  Block 483 (52 nodes):\n    Dominant community: 4 (98.1%)\n    Distribution: {3: 1, 4: 51}\n\n  Block 493 (25 nodes):\n    Dominant community: 5 (64.0%)\n    Distribution: {1: 9, 5: 16}\n\n  Block 512 (61 nodes):\n    Dominant community: 3 (95.1%)\n    Distribution: {2: 2, 3: 58, 5: 1}\n\n  Block 517 (20 nodes):\n    Dominant community: 3 (50.0%)\n    Distribution: {2: 5, 3: 10, 5: 5}\n\n  Block 525 (14 nodes):\n    Dominant community: 3 (64.3%)\n    Distribution: {2: 4, 3: 9, 5: 1}\n\n  Block 535 (14 nodes):\n    Dominant community: 5 (42.9%)\n    Distribution: {1: 3, 4: 5, 5: 6}\n\n  Block 540 (19 nodes):\n    Dominant community: 2 (100.0%)\n    Distribution: {2: 19}\n\n  Block 542 (3 nodes):\n    Dominant community: 4 (100.0%)\n    Distribution: {4: 3}\n\n  Block 554 (39 nodes):\n    Dominant community: 2 (100.0%)\n    Distribution: {2: 39}\n\n  Block 565 (23 nodes):\n    Dominant community: 3 (56.5%)\n    Distribution: {3: 13, 4: 1, 5: 9}\n\n  Block 567 (22 nodes):\n    Dominant community: 4 (50.0%)\n    Distribution: {1: 4, 2: 5, 4: 11, 5: 2}\n\n  Block 610 (11 nodes):\n    Dominant community: 1 (100.0%)\n    Distribution: {1: 11}\n\n  Block 624 (23 nodes):\n    Dominant community: 3 (69.6%)\n    Distribution: {1: 6, 2: 1, 3: 16}\n\n  Block 649 (25 nodes):\n    Dominant community: 3 (36.0%)\n    Distribution: {1: 9, 2: 4, 3: 9, 5: 3}\n\n  Block 665 (10 nodes):\n    Dominant community: 2 (100.0%)\n    Distribution: {2: 10}\n\n  Block 673 (11 nodes):\n    Dominant community: 1 (81.8%)\n    Distribution: {1: 9, 2: 2}\n\n  Block 855 (10 nodes):\n    Dominant community: 4 (80.0%)\n    Distribution: {2: 2, 4: 8}\n\n  Block 859 (10 nodes):\n    Dominant community: 1 (100.0%)\n    Distribution: {1: 10}\n\n  Block 886 (37 nodes):\n    Dominant community: 2 (100.0%)\n    Distribution: {2: 37}\n\n  Block 896 (47 nodes):\n    Dominant community: 1 (95.7%)\n    Distribution: {1: 45, 5: 2}\n\n  Block 921 (46 nodes):\n    Dominant community: 4 (56.5%)\n    Distribution: {2: 19, 3: 1, 4: 26}\n\n  Block 942 (10 nodes):\n    Dominant community: 2 (70.0%)\n    Distribution: {2: 7, 4: 1, 5: 2}\n\n  Block 950 (23 nodes):\n    Dominant community: 2 (100.0%)\n    Distribution: {2: 23}\n\n  Block 965 (22 nodes):\n    Dominant community: 4 (45.5%)\n    Distribution: {1: 5, 3: 5, 4: 10, 5: 2}\n\n  Block 995 (23 nodes):\n    Dominant community: 2 (95.7%)\n    Distribution: {2: 22, 4: 1}\n```\n:::\n:::\n\n\n## 11. Visualizing the Hierarchical Block Model\n\nThe nested SBM can be visualized in multiple ways:\n1. **Hierarchical tree**: Shows the nested community structure\n2. **Network with block colors**: Traditional network layout colored by blocks\n3. **Block matrix**: Edge density between blocks\n\n::: {#sbm-viz-hierarchy .cell execution_count=28}\n``` {.python .cell-code}\nif GRAPH_TOOL_AVAILABLE:\n    import matplotlib.cm\n    print(\"Drawing hierarchical block model structure...\")\n\n    # Draw the nested block model hierarchy with edge weights\n    # Edge color and width are mapped to weight using log scale\n    nested_state.draw(\n        edge_color=gt.prop_to_size(e_weight, power=1, log=True),\n        ecmap=(matplotlib.cm.inferno, 0.6),\n        eorder=e_weight,\n        edge_pen_width=gt.prop_to_size(e_weight, 1, 4, power=1, log=True),\n        edge_gradient=[],\n        vertex_size=3,\n        output_size=(1200, 1000),\n        output=\"../../../data/beerAdvocate/sbm_hierarchy.png\"\n    )\n    print(\"Saved hierarchical visualization to: sbm_hierarchy.png\")\n\n    # Display in notebook\n    from IPython.display import Image\n    Image(filename='../../../data/beerAdvocate/sbm_hierarchy.png')\nelse:\n    print(\"Skipping SBM visualization (graph-tool not installed)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDrawing hierarchical block model structure...\nSaved hierarchical visualization to: sbm_hierarchy.png\n```\n:::\n:::\n\n\n::: {#cell-sbm-matrix .cell execution_count=29}\n``` {.python .cell-code}\nif GRAPH_TOOL_AVAILABLE:\n    # Visualize the block model structure at bottom level\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    # Get unique block IDs that actually exist\n    unique_blocks = sorted(block_counts.keys())\n    block_to_idx = {b: i for i, b in enumerate(unique_blocks)}\n    n_blocks_viz = len(unique_blocks)\n\n    # Create block matrix (edge counts between blocks)\n    block_matrix = np.zeros((n_blocks_viz, n_blocks_viz))\n\n    for e in g.edges():\n        b1, b2 = blocks[e.source()], blocks[e.target()]\n        if b1 in block_to_idx and b2 in block_to_idx:\n            i1, i2 = block_to_idx[b1], block_to_idx[b2]\n            block_matrix[i1, i2] += e_weight[e]\n            if i1 != i2:\n                block_matrix[i2, i1] += e_weight[e]\n\n    # Normalize by block sizes for probability\n    block_sizes = np.array([block_counts[b] for b in unique_blocks])\n    block_probs = block_matrix / np.outer(block_sizes, block_sizes)\n\n    # Plot\n    im = ax.imshow(block_probs, cmap='YlOrRd')\n    ax.set_xlabel('Block', fontsize=12)\n    ax.set_ylabel('Block', fontsize=12)\n    ax.set_title('Hierarchical SBM: Edge Density Between Bottom-Level Blocks', fontsize=14)\n\n    # Only show tick labels if not too many blocks\n    if n_blocks_viz <= 20:\n        ax.set_xticks(range(n_blocks_viz))\n        ax.set_yticks(range(n_blocks_viz))\n        ax.set_xticklabels(unique_blocks)\n        ax.set_yticklabels(unique_blocks)\n\n    plt.colorbar(im, ax=ax, label='Edge Density')\n    plt.tight_layout()\n    plt.savefig('../../../data/beerAdvocate/block_matrix.png', dpi=150)\n    plt.show()\n    print(\"Saved: block_matrix.png\")\nelse:\n    print(\"Skipping block matrix visualization (graph-tool not installed)\")\n```\n\n::: {.cell-output .cell-output-display}\n![Block model edge probability matrix](network_analysis_files/figure-html/sbm-matrix-output-1.png){#sbm-matrix width=866 height=757}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSaved: block_matrix.png\n```\n:::\n:::\n\n\n::: {#final-comparison .cell execution_count=30}\n``` {.python .cell-code}\nif GRAPH_TOOL_AVAILABLE:\n    print(\"\\n\" + \"=\" * 70)\n    print(\"COMMUNITY DETECTION COMPARISON\")\n    print(\"=\" * 70)\n\n    # Calculate SBM modularity for comparison using bottom-level blocks\n    # Use actual block IDs from block_counts (they may not be contiguous)\n    sbm_communities = []\n    for block_id in block_counts.keys():\n        comm = set()\n        for v in g.vertices():\n            if blocks[v] == block_id:\n                comm.add(v_id[v])\n        if comm:\n            sbm_communities.append(comm)\n\n    # Verify we have a valid partition before calculating modularity\n    all_sbm_nodes = set().union(*sbm_communities) if sbm_communities else set()\n    if len(all_sbm_nodes) == G_users.number_of_nodes():\n        sbm_modularity = nx.community.modularity(G_users, sbm_communities)\n    else:\n        print(f\"  Warning: SBM partition covers {len(all_sbm_nodes)} of {G_users.number_of_nodes()} nodes\")\n        sbm_modularity = float('nan')\n\n    print(f\"\\n{'Algorithm':<30} {'Communities':>12} {'Modularity':>12}\")\n    print(\"-\" * 55)\n    print(f\"{'Louvain':<30} {len(louvain_communities):>12} {louvain_modularity:>12.4f}\")\n    print(f\"{'Label Propagation':<30} {len(label_prop_communities):>12} {label_prop_modularity:>12.4f}\")\n    print(f\"{'Greedy Modularity':<30} {len(greedy_communities):>12} {greedy_modularity:>12.4f}\")\n    sbm_mod_str = f\"{sbm_modularity:>12.4f}\" if not np.isnan(sbm_modularity) else \"         N/A\"\n    print(f\"{'Nested SBM (bottom level)':<30} {n_blocks:>12} {sbm_mod_str}\")\n    print(\"-\" * 55)\n\n    print(f\"\\nTrue communities (from data): 5\")\n    print(f\"\\nThe hierarchical nested SBM provides:\")\n    print(f\"  - {len(levels)} levels of hierarchy\")\n    print(f\"  - Bayesian model selection (no need to specify # of communities)\")\n    print(f\"  - Description length: {nested_state.entropy():.2f} (lower = better fit)\")\n    print(f\"  - Multi-scale community structure discovery\")\n    print(\"=\" * 70)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n======================================================================\nCOMMUNITY DETECTION COMPARISON\n======================================================================\n\nAlgorithm                       Communities   Modularity\n-------------------------------------------------------\nLouvain                                   5       0.2742\nLabel Propagation                         1       0.0000\nGreedy Modularity                         4       0.1387\nNested SBM (bottom level)                45       0.0436\n-------------------------------------------------------\n\nTrue communities (from data): 5\n\nThe hierarchical nested SBM provides:\n  - 11 levels of hierarchy\n  - Bayesian model selection (no need to specify # of communities)\n  - Description length: 653166.22 (lower = better fit)\n  - Multi-scale community structure discovery\n======================================================================\n```\n:::\n:::\n\n\n## Conclusion\n\nThis analysis demonstrates the workflow for:\n\n1. **Loading and constructing** a two-mode (bipartite) network from forum participation data\n2. **Projecting** the network onto users to reveal attention-based similarity\n3. **Analyzing** network properties including small-world characteristics\n4. **Detecting communities** using multiple algorithms (Louvain, Label Propagation, Greedy Modularity)\n5. **Applying hierarchical nested stochastic block models** for multi-scale probabilistic community detection\n6. **Visualizing** the network structure, community assignments, and hierarchical block model\n\nThe **hierarchical nested SBM** offers several advantages over flat community detection:\n\n- **Multi-scale discovery**: Reveals community structure at multiple resolutions simultaneously\n- **Bayesian model selection**: Automatically determines the optimal number of communities without user specification\n- **Edge weight support**: Natively incorporates connection strengths into the inference\n- **Hierarchical interpretation**: Shows how fine-grained communities nest within larger groupings\n\nThe results show that beer enthusiasts form distinct communities based on their attention patterns—engaging primarily with styles from their preferred family (British, American Craft, Belgian, Dark, or Lager) while some \"bridge\" users connect across communities. The hierarchical structure may reveal sub-communities within these major groupings.\n\nFor Fuller's market research, these attention-based segments offer insights that traditional preference surveys cannot capture: they reveal not what consumers claim to like, but where they actually direct their engagement and curiosity.\n\n",
    "supporting": [
      "network_analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}